{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retention study analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data set preparation and XGboost Fit\n",
    "\n",
    "Data set is built with retention_study_data_set_gen.ipynb\n",
    "\n",
    "Create train/validation/test split on data. Right now doing single validation set, but could consider expanding to k-fold cross validation to tune one of the hyperparameters like scale positive weight or the prediction lead time. If we do this with lead time, this will require multiple versions of the data set to be built.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable nomenclature\n",
    "\n",
    "summary where * is train/test/val, N = number of month observations, F = number of features, P = number of physicians\n",
    "| Variable | Description|\n",
    "|----------|-------------|\n",
    "| X_*       | (N x F) DataFrame of features|\n",
    "| y_*       |(N x 1) Series of binary depart within interval|\n",
    "| X_*_ids   |(N x 1) Series of physician ids for each month|\n",
    "| *_ids     |(P x 2) Dataframe with [0,:] list of physician ids, [1,:] binary depart within study for physician |\n",
    "| y_*_pred  |(N x1) series of binary predicitons |\n",
    "| y_*_pred_prob | (Nx1) series of raw xgboost output |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from matplotlib import pyplot as plt\n",
    "import shap\n",
    "from copy import deepcopy\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import balanced_accuracy_score, roc_auc_score, make_scorer, confusion_matrix, plot_confusion_matrix\n",
    "from sklearn.metrics import classification_report, auc, roc_curve, precision_recall_curve, f1_score\n",
    "from sklearn.base import clone as skClone\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "SEED = 0\n",
    "np.random.seed(SEED)\n",
    "pd.set_option('display.max_columns', None)\n",
    "PATH = './data/processed/turbo_7_29_22_deid_processed_3_ROUND_5y_TENURE_NO_STUDYDAY.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns_impute = [\n",
    "    'age_group', \n",
    "    'tenure', \n",
    "    'number_of_rx_errors',\n",
    "    'EWA_avg_number_of_rx_errors'\n",
    "]\n",
    "numeric_columns_impute = ['physician_demand',\n",
    "    'teamwork_on_inbox_value',\n",
    "    'r_slope_note_quality_manual_value',\n",
    "    'r_slope_note_time_8',\n",
    "    'r_slope_order_time_8',\n",
    "    'wow_time_8',\n",
    "    'review_time_8',\n",
    "    'r_slope_ib_time_8',\n",
    "    'EWA_avg_wow_time_8',\n",
    "    'r_slope_patient_volume',\n",
    "    'risk_avg',\n",
    "    'EWA_avg_note_quality_manual_value',\n",
    "    'r_slope_panel_cnt',\n",
    "    'r_slope_note_quality_contribution_value',\n",
    "    'physician_work_intensity',\n",
    "    'EWA_avg_risk_avg',\n",
    "    'r_slope_ehr_time_8',\n",
    "    'r_slope_review_time_8',\n",
    "    'panel_cnt',\n",
    "    'note_time_8',\n",
    "    'r_slope_physician_work_intensity',\n",
    "    'ib_time_8',\n",
    "    'r_slope_physician_demand',\n",
    "    'r_slope_teamwork_on_inbox_value',\n",
    "    'EWA_avg_note_quality_contribution_value',\n",
    "    'EWA_avg_panel_cnt',\n",
    "    'EWA_avg_teamwork_on_inbox_value',\n",
    "    'r_slope_risk_avg',\n",
    "    'ehr_time_8',\n",
    "    'EWA_avg_order_time_8',\n",
    "    'note_quality_contribution_value',\n",
    "    'EWA_avg_ib_time_8',\n",
    "    'note_quality_manual_value',\n",
    "    'EWA_avg_note_time_8',\n",
    "    'r_slope_wow_time_8',\n",
    "    'order_time_8',\n",
    "    'r_slope_number_of_rx_errors'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pipe():\n",
    "    # found here: https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html\n",
    "    numeric_transformer = Pipeline(\n",
    "        steps=[(\"imputer\", SimpleImputer(strategy=\"median\"))]\n",
    "    )\n",
    "    categorical_transformer = Pipeline(\n",
    "        steps=[(\"imputer\", SimpleImputer(strategy=\"most_frequent\"))]\n",
    "    )\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, numeric_columns_impute),\n",
    "            (\"cat\", categorical_transformer, categorical_columns_impute),\n",
    "        ],\n",
    "        remainder='passthrough',\n",
    "    )\n",
    "    # pipeline for the model\n",
    "    pipe = Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocessor\", preprocessor),        \n",
    "        ]\n",
    "    )\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    categorical_cols = [\n",
    "    'physician_id',\n",
    "    'age_group',\n",
    "    'gender',\n",
    "    'departure_in_interval',\n",
    "    'calendar_month',\n",
    "    'covid_wave'\n",
    "    ]\n",
    "    ehr_data = pd.read_pickle(path)\n",
    "    ehr_data = ehr_data.drop(['provtype_Physician','reportingperiodstartdate'],axis=1)\n",
    "    mask = (ehr_data['specialty_Family Medicine']==0) & (ehr_data['specialty_Internal Medicine']==0) & (ehr_data['specialty_Pediatrics']==0)\n",
    "    # Apply mask to the data so we can NaN the panel count and panel complexity on specialty that is not\n",
    "    # specialty_Family, specialty_Internal, specialty_Pediatrics\n",
    "    ehr_data.loc[mask, 'panel_cnt'] = np.nan\n",
    "    ehr_data.loc[mask, 'risk_avg'] = np.nan\n",
    "    all_ids = pd.DataFrame({\n",
    "        'id':  ehr_data['physician_id'].unique(),\n",
    "        'depart': ehr_data.groupby('physician_id')['departure_in_interval'].any().tolist()\n",
    "    })\n",
    "    ID_train, ID_test = train_test_split(ehr_data['physician_id'].unique(),\n",
    "                                                test_size=0.2, random_state=SEED)\n",
    "    assert len(set(ID_train).intersection(set(ID_test))) == 0, 'Bad split'\n",
    "    X_train = ehr_data[ehr_data['physician_id'].isin(ID_train)].drop(['departure_in_interval', 'physician_id'], axis=1)\n",
    "    X_test = ehr_data[ehr_data['physician_id'].isin(ID_test)].drop(['departure_in_interval', 'physician_id'], axis=1)\n",
    "    y_train = ehr_data[ehr_data['physician_id'].isin(ID_train)]['departure_in_interval']\n",
    "    y_test = ehr_data[ehr_data['physician_id'].isin(ID_test)]['departure_in_interval']\n",
    "    X_save = ehr_data\n",
    "    y_save = ehr_data.pop('departure_in_interval')\n",
    "    X_save_ids = ehr_data.pop('physician_id')\n",
    "    save_ids = all_ids\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, X_save, y_save, X_save_ids, save_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data2(path, impute=False):\n",
    "    '''\n",
    "    Looks like I was given some data that has been preprocessed and has the specialty mask already enabled, only issue\n",
    "    is that I cant split based on id, so going to do my best guess\n",
    "    '''\n",
    "    categorical_cols = [\n",
    "    'age_group',\n",
    "    'gender',\n",
    "    'departure_in_interval',\n",
    "    'calendar_month',\n",
    "    'covid_wave'\n",
    "    ]\n",
    "    X,y = pd.read_pickle(path)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=SEED)\n",
    "    if impute:\n",
    "        pipe = get_pipe()\n",
    "        X_train = pipe.fit_transform(X_train)\n",
    "        X_test = pipe.transform(X_test)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test, X_save, y_save, X_save_ids, save_ids = load_data(PATH)\n",
    "# This impute == true is to deal with data that has NaNs, same can be done for the \n",
    "# original loader but probs best to merge the two\n",
    "X_train, X_test, y_train, y_test = load_data2('masked_for_kevin.pkl', impute=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance Evaluation Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.rcParams[\"figure.figsize\"] = (10,7)\n",
    "def model_perf(classifier, X_train, X_test, y_train, y_test,crosstab=True, stats = True, roc_plot = True, optimal_thresh=True, custom_thresh = None, conf_interval=False, bootstrap_nsamples = 200):\n",
    "    # simple helper to easily show some key features of performace\n",
    "    X = X_test\n",
    "    y = y_test\n",
    "    y_pred = classifier.predict(X)\n",
    "    probs = classifier.predict_proba(X)\n",
    "    scores = probs[:,1]\n",
    "    fpr, tpr, threshold = roc_curve(y, scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    if crosstab:\n",
    "        display(pd.crosstab(y,y_pred))\n",
    "    \n",
    "    if stats:\n",
    "        print(classification_report(y, y_pred))\n",
    "        print(get_stats(y, y_pred))\n",
    "\n",
    "    # method I: plt\n",
    "    if roc_plot:\n",
    "        plt.title('Main Results ROC Curve and AUC')\n",
    "        _ROC = plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "        #plt.legend(loc = 'lower right')\n",
    "        plt.plot([0, 1], [0, 1],'r--')\n",
    "        plt.xlim([0, 1])\n",
    "        plt.ylim([0, 1])\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        #plt.show()\n",
    "    if optimal_thresh:\n",
    "        opt_cutoff, ix = cutoff_youdens_j(fpr, tpr, threshold)#Find_Optimal_Cutoff(y, scores)[0]\n",
    "        print('Optimal Threshold cutoff')\n",
    "        #print(opt_cutoff)\n",
    "        #ix = gmean(fpr, tpr, threshold)\n",
    "        plt.plot(fpr[ix], tpr[ix], marker='o', color='black', label='Best Threshold (Youdens J Stat) =%f' % (opt_cutoff))\n",
    "        #plt.text(2,4,'This text starts at point (2,4)')\n",
    "        plt.vlines(fpr[ix], 0, 1, linestyles='dashed', color='black')\n",
    "        print(classification_report(y, scores > opt_cutoff))\n",
    "        display(pd.crosstab(y,scores > opt_cutoff))\n",
    "        print(get_stats(y, scores > opt_cutoff))\n",
    "    if custom_thresh is not None:\n",
    "        for ct in custom_thresh:\n",
    "            print('Custom Thresh')\n",
    "            print(ct)\n",
    "            print(classification_report(y, scores > ct))\n",
    "            display(pd.crosstab(y,scores > ct))\n",
    "            print(get_stats(y, scores > ct))\n",
    "    if conf_interval:\n",
    "        bootstrap_stats = bootstrap_auc(classifier, X_train, y_train, X_test, y_test, nsamples=bootstrap_nsamples)\n",
    "        roc_aucs = []\n",
    "        roc_aucs = []\n",
    "        pr_aucs = []\n",
    "        for i in range(len(bootstrap_stats)):\n",
    "            roc_aucs.append(bootstrap_stats[i]['roc_auc'])\n",
    "            pr_aucs.append(bootstrap_stats[i]['pr_auc'])\n",
    "        roc_CI = np.percentile(roc_aucs, (2.5, 97.5))\n",
    "        pr_CI = np.percentile(pr_aucs, (2.5, 97.5))\n",
    "        plt.fill_between(fpr, (tpr-(roc_CI[1]- roc_CI[0])), (tpr+(roc_CI[1]- roc_CI[0])), alpha=0.2)\n",
    "        _ROC[0].set_label(f'AUC = {roc_auc:.2f} CI=[{roc_CI[0]:.2f} - {roc_CI[1]:.2f}]')\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.show()\n",
    "def cutoff_youdens_j(fpr,tpr,thresholds):\n",
    "    j_scores = tpr-fpr\n",
    "    ix = np.argmax(j_scores)\n",
    "    best_thresh = thresholds[ix]\n",
    "    print('Best Threshold (Youdens J Stat)=%f' % (best_thresh))\n",
    "    j_ordered = sorted(zip(j_scores,thresholds))\n",
    "    return j_ordered[-1][1], ix\n",
    "\n",
    "def gmean(fpr,tpr,thresholds):\n",
    "    # calculate the g-mean for each threshold\n",
    "    gmeans = np.sqrt(tpr * (1-fpr))\n",
    "    # locate the index of the largest g-mean\n",
    "    ix = np.argmax(gmeans)\n",
    "    print('Best GMeans Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))\n",
    "    return ix\n",
    "\n",
    "def perf_measure(y_actual, y_hat):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "    for i in range(len(y_hat)): \n",
    "        if y_actual[i]==y_hat[i]==1:\n",
    "            TP += 1\n",
    "        if y_hat[i]==1 and y_actual[i]!=y_hat[i]:\n",
    "            FP += 1\n",
    "        if y_actual[i]==y_hat[i]==0:\n",
    "            TN += 1\n",
    "        if y_hat[i]==0 and y_actual[i]!=y_hat[i]:\n",
    "            FN += 1\n",
    "    return(TP, FP, TN, FN)\n",
    "def get_stats(true_value, classifier_output):\n",
    "    # we need sensitivity, specificity, npv, ppv for all 3 thresholds\n",
    "    # Note that in binary classification, recall of the positive class \n",
    "    # is also known as “sensitivity”; recall of the negative class is “specificity”.\n",
    "    TN, FP, FN, TP = confusion_matrix(true_value, classifier_output).ravel() #perf_measure(true_value, classifier_output)# confusion_matrix(true_value, classifier_output).ravel()\n",
    "    ppv = TP/(TP+FP)\n",
    "    npv = TN/(TN+FN)\n",
    "    specificity = TN/(TN+FP)\n",
    "    sensitivity = TP/(TP+FN)\n",
    "    return {'ppv': ppv, 'npv': npv, 'specificity': specificity, 'sensitivity': sensitivity}\n",
    "    \n",
    "# configure bootstrap\n",
    "def bootstrap_auc(fit_clf, X_train, y_train, X_test, y_test, nsamples=1000):\n",
    "    X_train = _numpy_to_df(X_train)\n",
    "    y_train = _numpy_to_df(y_train)\n",
    "    X_test = _numpy_to_df(X_test)\n",
    "    #y_test = _numpy_to_df(y_test)\n",
    "    statistics = {}\n",
    "    for b in range(nsamples):\n",
    "        clf = skClone(fit_clf)# gives us a unfit clone of the classifier\n",
    "        idx = np.random.randint(X_train.shape[0], size=X_train.shape[0])\n",
    "        clf.fit(X_train.iloc[idx], y_train.iloc[idx].values.ravel())# strange that we needed to ravel the values after? something to do with pandas and numpy\n",
    "        pred = clf.predict_proba(X_test)[:, 1]\n",
    "        #roc_auc = roc_auc_score(y_test.ravel(), pred.ravel())\n",
    "        fpr, tpr, threshold = roc_curve(y_test.ravel(), pred.ravel())\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        precision, recall, thresholds = precision_recall_curve(y_test.ravel(), pred.ravel())\n",
    "        pr_auc = auc(recall, precision)\n",
    "        statistics[b] = {'roc_auc': roc_auc, 'fpr': fpr, 'tpr': tpr, 'precision': precision, 'recall': recall, 'pr_auc':pr_auc}\n",
    "        \n",
    "    return statistics#np.percentile(auc_values, (2.5, 97.5))\n",
    "def _numpy_to_df(df):\n",
    "    if type(df) == np.ndarray:\n",
    "        return pd.DataFrame(deepcopy(df))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# dummy Data: \n",
    "from sklearn.datasets import make_classification\n",
    "def get_dummy_data(random_state = 0):\n",
    "    X, y = make_classification(n_samples=2000, n_features=25, random_state=random_state, class_sep=0.25)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    return X_train, X_test, y_train, y_test'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# import baseline models:\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "def trainBaselines(model, X_train, X_test, y_train, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    model_pred = model.predict(X_test)\n",
    "    model_prob = model.predict_proba(X_test)\n",
    "    model_perf(model,X_train, X_test, y_train, y_test,stats=True,roc_plot=True, optimal_thresh=True, crosstab=True, conf_interval=True, bootstrap_nsamples=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline -- Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "LR = LogisticRegression(random_state=SEED)\n",
    "#X_train, X_test, y_train, y_test = get_dummy_data()# THIS LINE SHOULD BE REMOVED TO USE REAL DATA!\n",
    "trainBaselines(LR, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline -- Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "#X_train, X_test, y_train, y_test = get_dummy_data()\n",
    "trainBaselines(gnb, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline -- Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "DTC = tree.DecisionTreeClassifier()\n",
    "#X_train, X_test, y_train, y_test = get_dummy_data()\n",
    "trainBaselines(DTC, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline -- Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "RFC = RandomForestClassifier(random_state=SEED)\n",
    "#X_train, X_test, y_train, y_test = get_dummy_data()\n",
    "trainBaselines(RFC, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline -- XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_xgb = xgb.XGBClassifier(\n",
    "    objective = 'binary:logistic',\n",
    "    seed = SEED,\n",
    ")\n",
    "#X_train, X_test, y_train, y_test = get_dummy_data()\n",
    "trainBaselines(classify_xgb, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridsearchCV -- XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "def f1_eval(y_pred, dtrain):\n",
    "    y_true = dtrain.get_label()\n",
    "    err = 1-f1_score(y_true, np.round(y_pred), average=None)\n",
    "    return 'f1_err', err'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_xgb = xgb.XGBClassifier(\n",
    "    objective = 'binary:logistic',\n",
    "    #missing = nan,\n",
    "    seed = SEED,\n",
    "    scale_pos_weight = 400, # approx \n",
    "    n_estimators=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {#'nthread':[2], #when use hyperthread, xgboost may become slower\n",
    "              'objective':['binary:logistic'],\n",
    "              'learning_rate': [0.3, 0.4, 0.5], #so called `eta` value\n",
    "              #'max_depth': [x for x in range(4, 10)],\n",
    "              #'reg_lambda':[1,10,20,40]\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_xgb_GS = GridSearchCV(classify_xgb, parameters, n_jobs=1, \n",
    "                                scoring='roc_auc',#make_scorer(f1_score, average='binary'),#'roc_auc',\n",
    "                                cv=5,\n",
    "                                verbose=2, refit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "classify_xgb_GS.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classify_xgb_GS.best_params_)\n",
    "print(classify_xgb_GS.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = classify_xgb_GS.predict(X_test)\n",
    "y_test_pred_prob = classify_xgb_GS.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classify_xgb_save = classify_xgb_GS.best_estimator_.fit(X_train,y_train)\n",
    "classify_xgb_save = classify_xgb_GS.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_perf(classify_xgb_GS,X_train, X_test, y_train, y_test,stats=True,roc_plot=True, optimal_thresh=True, crosstab=True)# custom_thresh=[0.8, 0.001]\n",
    "# can add: conf_interval=True, bootstrap_nsamples=50) to get the band and the CI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets store results\n",
    "import pickle\n",
    "train_list = [classify_xgb_save,X_save,y_save,X_save_ids,save_ids]\n",
    "fpath = './models/xgb_classifier_train_test_without_specialty_5y.pkl'\n",
    "with open(fpath,\"wb\") as open_file:\n",
    "    pickle.dump(train_list,open_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Roc Curve\n",
    "fpr, tpr, threshold = roc_curve(y_test, y_test_pred_prob[:,1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "# precision recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_test_pred_prob[:,1])\n",
    "# calculate precision-recall AUC\n",
    "pr_auc = auc(recall, precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "bootstrap_stats = bootstrap_auc(classify_xgb_GS.best_estimator_, X_train, y_train, X_test, y_test, nsamples=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_aucs = []\n",
    "pr_aucs = []\n",
    "for i in range(len(bootstrap_stats)):\n",
    "    roc_aucs.append(bootstrap_stats[i]['roc_auc'])\n",
    "    pr_aucs.append(bootstrap_stats[i]['pr_auc'])\n",
    "roc_CI = np.percentile(roc_aucs, (2.5, 97.5))\n",
    "pr_CI = np.percentile(pr_aucs, (2.5, 97.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2,figsize=(15,5))\n",
    "ax1.set_title('Receiver Operating Characteristic')\n",
    "ax1.plot(fpr, tpr, 'b', label = 'AUC = {:.2f} [CI={:.3f},{:.3f}]'.format(roc_auc, roc_CI[0], roc_CI[1]))\n",
    "#plt.plot(bootstrap_stats[maxIDX]['fpr'], bootstrap_stats[maxIDX]['tpr'], 'r')\n",
    "#plt.plot(bootstrap_stats[minIDX]['fpr'], bootstrap_stats[minIDX]['tpr'], 'r')\n",
    "ax1.fill_between(fpr, (tpr-(roc_CI[1]- roc_CI[0])), (tpr+(roc_CI[1]- roc_CI[0])), alpha=0.2)\n",
    "ax1.legend(loc = 'lower right')\n",
    "ax1.plot([0, 1], [0, 1],'r--')\n",
    "ax1.set_xlim([0, 1])\n",
    "ax1.set_ylim([0, 1])\n",
    "ax1.set_ylabel('True Positive Rate')\n",
    "ax1.set_xlabel('False Positive Rate')\n",
    "ax2.set_title('Precision Recall Curve')\n",
    "ax2.plot(recall, precision, 'b', label = 'AUC = {:.2f} [CI={:.3f},{:.3f}]'.format(pr_auc, pr_CI[0], pr_CI[1]))\n",
    "# fill between\n",
    "ax2.fill_between(recall, (precision-(pr_CI[1]- pr_CI[0])), (precision+(pr_CI[1]- pr_CI[0])), alpha=0.2)\n",
    "# calculate the no skill line as the proportion of the positive class\n",
    "no_skill = len(y_test[y_test==1]) / len(y_test)# Essentially the fraction of positive classes/total number of examples\n",
    "# plot the no skill precision-recall curve\n",
    "ax2.plot([0, 1], [no_skill, no_skill], 'r--', label='No Skill = %0.2f' % no_skill)\n",
    "ax2.legend(loc = 'best')\n",
    "ax2.set_xlim([0, 1])\n",
    "ax2.set_ylim([0, 1])\n",
    "ax2.set_xlabel('Recall')\n",
    "ax2.set_ylabel('Precision')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ehr_data_ = pd.read_pickle('./data/processed/turbo_7_29_22_deid_processed_3_ROUND_5y_TENURE_NO_STUDYDAY.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ehr_data_[ehr_data_['departure_in_interval'] == True]['physician_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "44/len(ehr_data_['physician_id'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aebeff7af16a3e74dfb8d2ca55a8624561b514dde17729b916609a121bafd8b7"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
