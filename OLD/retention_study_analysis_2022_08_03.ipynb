{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retention study analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from matplotlib import pyplot as plt\n",
    "import shap\n",
    "from copy import deepcopy\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import balanced_accuracy_score, roc_auc_score, make_scorer, confusion_matrix, plot_confusion_matrix\n",
    "from sklearn.metrics import classification_report, auc, roc_curve, precision_recall_curve, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "SEED = 0\n",
    "np.random.seed(SEED)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data set preparation and XGboost Fit\n",
    "\n",
    "Data set is built with retention_study_data_set_gen.ipynb\n",
    "\n",
    "Create train/validation/test split on data. Right now doing single validation set, but could consider expanding to k-fold cross validation to tune one of the hyperparameters like scale positive weight or the prediction lead time. If we do this with lead time, this will require multiple versions of the data set to be built.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable nomenclature\n",
    "\n",
    "summary where * is train/test/val, N = number of month observations, F = number of features, P = number of physicians\n",
    "| Variable | Description|\n",
    "|----------|-------------|\n",
    "| X_*       | (N x F) DataFrame of features|\n",
    "| y_*       |(N x 1) Series of binary depart within interval|\n",
    "| X_*_ids   |(N x 1) Series of physician ids for each month|\n",
    "| *_ids     |(P x 2) Dataframe with [0,:] list of physician ids, [1,:] binary depart within study for physician |\n",
    "| y_*_pred  |(N x1) series of binary predicitons |\n",
    "| y_*_pred_prob | (Nx1) series of raw xgboost output |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ehr_data = pd.read_pickle('./data/processed/turbo_7_29_22_deid_processed_3_ROUND_5y_TENURE_NO_STUDYDAY.pkl')\n",
    "ehr_data = ehr_data.drop(['provtype_Physician','reportingperiodstartdate'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ehr_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (ehr_data['specialty_Family Medicine']==0) & (ehr_data['specialty_Internal Medicine']==0) & (ehr_data['specialty_Pediatrics']==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply mask to the data so we can NaN the panel count and panel complexity on specialty that is not\n",
    "# specialty_Family, specialty_Internal, specialty_Pediatrics\n",
    "ehr_data.loc[mask, 'panel_cnt'] = np.nan\n",
    "ehr_data.loc[mask, 'risk_avg'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = [\n",
    "    'physician_id',\n",
    "    'age_group',\n",
    "    'gender',\n",
    "    'departure_in_interval',\n",
    "    'calendar_month',\n",
    "    'covid_wave'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ids = pd.DataFrame({\n",
    "    'id':  ehr_data['physician_id'].unique(),\n",
    "    'depart': ehr_data.groupby('physician_id')['departure_in_interval'].any().tolist()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make train, test, validation\n",
    "#   - 0.33 test set\n",
    "#   - 0.25 validation\n",
    "#   - perform on IDs to allow for stratification\n",
    "'''test_split = 0.33\n",
    "val_split = 0.25\n",
    "val_n = 10 # in case we want to tune hyperparas\n",
    "\n",
    "test_sss = StratifiedShuffleSplit(n_splits = 1, test_size = test_split, random_state = 0)\n",
    "val_sss = StratifiedShuffleSplit(n_splits = val_n, test_size = val_split, random_state = 0)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train+val / test split\n",
    "'''tv_idxs, test_idxs = next(test_sss.split(all_ids[['id']],all_ids['depart']))\n",
    "tv_ids, test_ids = all_ids.iloc[tv_idxs,:], all_ids.iloc[test_idxs,:]\n",
    "\n",
    "# train / val split\n",
    "train_idxs, val_idxs = next(val_sss.split(tv_ids[['id']],tv_ids['depart']))\n",
    "train_ids, val_ids = tv_ids.iloc[train_idxs,:], tv_ids.iloc[val_idxs,:]'''\n",
    "\n",
    "# train_id_status = id_status[id_status['id'].isin(pd.Series(train_ids))]\n",
    "# val_id_status = id_status[id_status['id'].isin(pd.Series(val_ids))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "assert ~any(tv_ids.id.isin(test_ids.id)), 'Bad train test split'\n",
    "assert ~any(train_ids.id.isin(val_ids.id)), 'Bad train val split'\n",
    "assert ~any(val_ids.id.isin(test_ids.id)), 'Bad val test split'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''X = ehr_data.drop(['departure_in_interval'], axis=1).copy()\n",
    "y = ehr_data['departure_in_interval'].copy()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_flag = X['physician_id'].isin(test_ids['id'])\n",
    "# val_flag= X['physician_id'].isin(val_ids['id'])\n",
    "# train_flag= X['physician_id'].isin(train_ids['id'])\n",
    "\n",
    "# X_test,y_test = X[test_flag], y[test_flag]\n",
    "# X_val,y_val = X[val_flag], y[val_flag]\n",
    "# X_train,y_train = X[train_flag], y[train_flag]\n",
    "\n",
    "# # extract the ids for double checking, drop from X\n",
    "# X_test_ids = X_test.pop('physician_id')\n",
    "# X_val_ids = X_val.pop('physician_id')\n",
    "# X_train_ids = X_train.pop('physician_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# positive fractions\n",
    "print(sum(ehr_data.departure_in_interval)/len(ehr_data))\n",
    "print(sum(y_test)/len(y_test))\n",
    "print(sum(y_val)/len(y_val))\n",
    "print(sum(y_train)/len(y_train))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''print(f'num depart months: {sum(y)}')\n",
    "print(f'total months: {len(y)}')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_train, ID_test = train_test_split(ehr_data['physician_id'].unique(),\n",
    "                                                test_size=0.2, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(set(ID_train).intersection(set(ID_test))) == 0, 'Bad split'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = ehr_data[ehr_data['physician_id'].isin(ID_train)].drop(['departure_in_interval', 'physician_id'], axis=1)\n",
    "X_test = ehr_data[ehr_data['physician_id'].isin(ID_test)].drop(['departure_in_interval', 'physician_id'], axis=1)\n",
    "y_train = ehr_data[ehr_data['physician_id'].isin(ID_train)]['departure_in_interval']\n",
    "y_test = ehr_data[ehr_data['physician_id'].isin(ID_test)]['departure_in_interval']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_save = ehr_data\n",
    "y_save = ehr_data.pop('departure_in_interval')\n",
    "X_save_ids = ehr_data.pop('physician_id')\n",
    "save_ids = all_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridsearchCV -- XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "def f1_eval(y_pred, dtrain):\n",
    "    y_true = dtrain.get_label()\n",
    "    err = 1-f1_score(y_true, np.round(y_pred), average=None)\n",
    "    return 'f1_err', err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_xgb = xgb.XGBClassifier(\n",
    "    objective = 'binary:logistic',\n",
    "    #missing = nan,\n",
    "    seed = SEED,\n",
    "    scale_pos_weight = 400, # approx \n",
    "    n_estimators=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {#'nthread':[2], #when use hyperthread, xgboost may become slower\n",
    "              'objective':['binary:logistic'],\n",
    "              'learning_rate': [0.3, 0.4, 0.5], #so called `eta` value\n",
    "              'max_depth': [x for x in range(4, 10)],\n",
    "              #'scale_pos_weight':[100,400],\n",
    "              'reg_lambda':[1,10,20,40]\n",
    "              #'min_child_weight': [11],\n",
    "              #'silent': [1],\n",
    "              #'subsample': [0.8],\n",
    "              #'colsample_bytree': [0.7],\n",
    "              #'n_estimators': [5], #number of trees, change it to 1000 for better results\n",
    "              #'missing':[-999],\n",
    "              #'eval_metric': ['auc', f1_eval],\n",
    "              #'seed': [SEED],\n",
    "              #'verbose':[False]\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_xgb_GS = GridSearchCV(classify_xgb, parameters, n_jobs=1, \n",
    "                                scoring='roc_auc',#make_scorer(f1_score, average='binary'),#'roc_auc',\n",
    "                                cv=5,\n",
    "                                verbose=2, refit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_xgb_GS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "classify_xgb_GS.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classify_xgb_GS.best_params_)\n",
    "print(classify_xgb_GS.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_xgb_GS.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = classify_xgb_GS.predict(X_test)\n",
    "y_test_pred_prob = classify_xgb_GS.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_xgb_save = classify_xgb_GS.best_estimator_.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions for all data sets\n",
    "\n",
    "'''y_test_pred = classify_xgb.predict(X_test)\n",
    "y_train_pred = classify_xgb.predict(X_train)\n",
    "y_val_pred = classify_xgb.predict(X_val)\n",
    "\n",
    "y_test_pred_prob = classify_xgb.predict_proba(X_test)\n",
    "y_train_pred_prob = classify_xgb.predict_proba(X_train)\n",
    "y_val_pred_prob = classify_xgb.predict_proba(X_val)'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance Evaluation script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.rcParams[\"figure.figsize\"] = (10,7)\n",
    "def model_perf(classifier, X, y,crosstab=True, stats = True, roc_plot = True, optimal_thresh=True, custom_thresh = None):\n",
    "    # simple helper to easily show some key features of performace\n",
    "    \n",
    "    y_pred = classifier.predict(X)\n",
    "    probs = classifier.predict_proba(X)\n",
    "    scores = probs[:,1]\n",
    "    fpr, tpr, threshold = roc_curve(y, scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    if crosstab:\n",
    "        display(pd.crosstab(y,y_pred))\n",
    "    \n",
    "    if stats:\n",
    "        print(classification_report(y, y_pred))\n",
    "        print(get_stats(y, y_pred))\n",
    "\n",
    "    # method I: plt\n",
    "    if roc_plot:\n",
    "        plt.title('Main Results ROC Curve and AUC')\n",
    "        plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "        #plt.legend(loc = 'lower right')\n",
    "        plt.plot([0, 1], [0, 1],'r--')\n",
    "        plt.xlim([0, 1])\n",
    "        plt.ylim([0, 1])\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        #plt.show()\n",
    "    if optimal_thresh:\n",
    "        opt_cutoff, ix = cutoff_youdens_j(fpr, tpr, threshold)#Find_Optimal_Cutoff(y, scores)[0]\n",
    "        print('Optimal Threshold cutoff')\n",
    "        #print(opt_cutoff)\n",
    "        #ix = gmean(fpr, tpr, threshold)\n",
    "        plt.plot(fpr[ix], tpr[ix], marker='o', color='black', label='Best Threshold (Youdens J Stat) =%f' % (opt_cutoff))\n",
    "        #plt.text(2,4,'This text starts at point (2,4)')\n",
    "        plt.vlines(fpr[ix], 0, 1, linestyles='dashed', color='black')\n",
    "        print(classification_report(y, scores > opt_cutoff))\n",
    "        display(pd.crosstab(y,scores > opt_cutoff))\n",
    "        print(get_stats(y, scores > opt_cutoff))\n",
    "    if custom_thresh is not None:\n",
    "        for ct in custom_thresh:\n",
    "            print('Custom Thresh')\n",
    "            print(ct)\n",
    "            print(classification_report(y, scores > ct))\n",
    "            display(pd.crosstab(y,scores > ct))\n",
    "            print(get_stats(y, scores > ct))\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.show()\n",
    "def cutoff_youdens_j(fpr,tpr,thresholds):\n",
    "    j_scores = tpr-fpr\n",
    "    ix = np.argmax(j_scores)\n",
    "    best_thresh = thresholds[ix]\n",
    "    print('Best Threshold (Youdens J Stat)=%f' % (best_thresh))\n",
    "    j_ordered = sorted(zip(j_scores,thresholds))\n",
    "    return j_ordered[-1][1], ix\n",
    "\n",
    "def gmean(fpr,tpr,thresholds):\n",
    "    # calculate the g-mean for each threshold\n",
    "    gmeans = np.sqrt(tpr * (1-fpr))\n",
    "    # locate the index of the largest g-mean\n",
    "    ix = np.argmax(gmeans)\n",
    "    print('Best GMeans Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))\n",
    "    return ix\n",
    "\n",
    "def perf_measure(y_actual, y_hat):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "    for i in range(len(y_hat)): \n",
    "        if y_actual[i]==y_hat[i]==1:\n",
    "            TP += 1\n",
    "        if y_hat[i]==1 and y_actual[i]!=y_hat[i]:\n",
    "            FP += 1\n",
    "        if y_actual[i]==y_hat[i]==0:\n",
    "            TN += 1\n",
    "        if y_hat[i]==0 and y_actual[i]!=y_hat[i]:\n",
    "            FN += 1\n",
    "    return(TP, FP, TN, FN)\n",
    "def get_stats(true_value, classifier_output):\n",
    "    # we need sensitivity, specificity, npv, ppv for all 3 thresholds\n",
    "    # Note that in binary classification, recall of the positive class \n",
    "    # is also known as “sensitivity”; recall of the negative class is “specificity”.\n",
    "    TN, FP, FN, TP = confusion_matrix(true_value, classifier_output).ravel() #perf_measure(true_value, classifier_output)# confusion_matrix(true_value, classifier_output).ravel()\n",
    "    ppv = TP/(TP+FP)\n",
    "    npv = TN/(TN+FN)\n",
    "    specificity = TN/(TN+FP)\n",
    "    sensitivity = TP/(TP+FN)\n",
    "    return {'ppv': ppv, 'npv': npv, 'specificity': specificity, 'sensitivity': sensitivity}\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_perf(classify_xgb,X_train,y_train,stats=False,roc_plot=False, optimal_thresh=False, crosstab=False)\n",
    "#model_perf(classify_xgb,X_val,y_val,stats=False,roc_plot=False, optimal_thresh=False, crosstab=False)\n",
    "model_perf(classify_xgb_GS,X_test,y_test,stats=True,roc_plot=True, optimal_thresh=True, crosstab=True)# custom_thresh=[0.8, 0.001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets store results\n",
    "import pickle\n",
    "# with open('./models/xgb_classifier_train_test_without_specialty.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "#     pickle.dump([classify_xgb_GS,X_train,y_train,X_test,y_test, y_test_pred, y_test_pred_prob], f)\n",
    "\n",
    "train_list = [classify_xgb_save,X_save,y_save,X_save_ids,save_ids]\n",
    "fpath = './models/xgb_classifier_train_test_without_specialty_5y.pkl'\n",
    "\n",
    "with open(fpath,\"wb\") as open_file:\n",
    "    pickle.dump(train_list,open_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Roc Curve\n",
    "fpr, tpr, threshold = roc_curve(y_test, y_test_pred_prob[:,1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "# precision recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_test_pred_prob[:,1])\n",
    "# calculate precision-recall AUC\n",
    "pr_auc = auc(recall, precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try bootstrap method:\n",
    "#found here: https://machinelearningmastery.com/calculate-bootstrap-confidence-intervals-machine-learning-results-python/\n",
    "# and: https://stackoverflow.com/questions/52373318/how-to-compare-roc-auc-scores-of-different-binary-classifiers-and-assess-statist\n",
    "from copy import deepcopy\n",
    "from sklearn.utils import resample\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "params = deepcopy(classify_xgb_GS.best_params_)\n",
    "# configure bootstrap\n",
    "def bootstrap_auc(X_train, y_train, X_test, y_test, nsamples=1000):\n",
    "    statistics = {}\n",
    "    for b in range(nsamples):\n",
    "        idx = np.random.randint(X_train.shape[0], size=X_train.shape[0])\n",
    "        clf = xgb.XGBClassifier(\n",
    "            objective = 'binary:logistic',\n",
    "            #seed = SEED,\n",
    "            scale_pos_weight = 400, # approx \n",
    "            n_estimators=200\n",
    "            )\n",
    "        clf.set_params(**params)\n",
    "        clf.fit(X_train.iloc[idx], y_train.iloc[idx], eval_metric='logloss')\n",
    "        pred = clf.predict_proba(X_test)[:, 1]\n",
    "        #roc_auc = roc_auc_score(y_test.ravel(), pred.ravel())\n",
    "        fpr, tpr, threshold = roc_curve(y_test.ravel(), pred.ravel())\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        precision, recall, thresholds = precision_recall_curve(y_test.ravel(), pred.ravel())\n",
    "        pr_auc = auc(recall, precision)\n",
    "        statistics[b] = {'roc_auc': roc_auc, 'fpr': fpr, 'tpr': tpr, 'precision': precision, 'recall': recall, 'pr_auc':pr_auc}\n",
    "        \n",
    "    return statistics#np.percentile(auc_values, (2.5, 97.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "bootstrap_stats = bootstrap_auc(X_train, y_train, X_test, y_test, nsamples=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_aucs = []\n",
    "pr_aucs = []\n",
    "for i in range(len(bootstrap_stats)):\n",
    "    roc_aucs.append(bootstrap_stats[i]['roc_auc'])\n",
    "    pr_aucs.append(bootstrap_stats[i]['pr_auc'])\n",
    "roc_CI = np.percentile(roc_aucs, (2.5, 97.5))\n",
    "pr_CI = np.percentile(pr_aucs, (2.5, 97.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2,figsize=(15,5))\n",
    "ax1.set_title('Receiver Operating Characteristic')\n",
    "ax1.plot(fpr, tpr, 'b', label = 'AUC = {:.2f} [CI={:.3f},{:.3f}]'.format(roc_auc, roc_CI[0], roc_CI[1]))\n",
    "#plt.plot(bootstrap_stats[maxIDX]['fpr'], bootstrap_stats[maxIDX]['tpr'], 'r')\n",
    "#plt.plot(bootstrap_stats[minIDX]['fpr'], bootstrap_stats[minIDX]['tpr'], 'r')\n",
    "ax1.fill_between(fpr, (tpr-(roc_CI[1]- roc_CI[0])), (tpr+(roc_CI[1]- roc_CI[0])), alpha=0.2)\n",
    "ax1.legend(loc = 'lower right')\n",
    "ax1.plot([0, 1], [0, 1],'r--')\n",
    "ax1.set_xlim([0, 1])\n",
    "ax1.set_ylim([0, 1])\n",
    "ax1.set_ylabel('True Positive Rate')\n",
    "ax1.set_xlabel('False Positive Rate')\n",
    "ax2.set_title('Precision Recall Curve')\n",
    "ax2.plot(recall, precision, 'b', label = 'AUC = {:.2f} [CI={:.3f},{:.3f}]'.format(pr_auc, pr_CI[0], pr_CI[1]))\n",
    "# fill between\n",
    "ax2.fill_between(recall, (precision-(pr_CI[1]- pr_CI[0])), (precision+(pr_CI[1]- pr_CI[0])), alpha=0.2)\n",
    "# calculate the no skill line as the proportion of the positive class\n",
    "no_skill = len(y_test[y_test==1]) / len(y_test)# Essentially the fraction of positive classes/total number of examples\n",
    "# plot the no skill precision-recall curve\n",
    "ax2.plot([0, 1], [no_skill, no_skill], 'r--', label='No Skill = %0.2f' % no_skill)\n",
    "ax2.legend(loc = 'best')\n",
    "ax2.set_xlim([0, 1])\n",
    "ax2.set_ylim([0, 1])\n",
    "ax2.set_xlabel('Recall')\n",
    "ax2.set_ylabel('Precision')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ehr_data_ = pd.read_pickle('./data/processed/turbo_7_29_22_deid_processed_3_ROUND_5y_TENURE_NO_STUDYDAY.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ehr_data_[ehr_data_['departure_in_interval'] == True]['physician_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "44/len(ehr_data_['physician_id'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expanded_crosstab(y,y_pred,id,id_status):\n",
    "    # expanded crosstab information showing classificatin performance by physician departure status\n",
    "    # y -> y_test (series)\n",
    "    # y_pred -> y_pred (series)\n",
    "    # ids -> X_test_ids (series)\n",
    "    # id_status -> test_ids (dataframe)\n",
    "\n",
    "    # test data for function validation \n",
    "    # y = pd.Series(np.array([False,False,False,False,False,False,False,False,True,True,False,True]))\n",
    "    # y_pred = pd.Series(np.array([False,False,False,True,False,False,False,False,True,True,False,True]))\n",
    "    # id = pd.Series(np.array([1,1,1,1,1,1,2,2,2,2,2,2]))\n",
    "    # id_status = pd.DataFrame({'id': [1,2],'depart': [False,True]})\n",
    "    # expanded_crosstab(y,y_pred,id,id_status)\n",
    "    # out:\n",
    "    #     class\ttotal\tnever dep\tdep\n",
    "    # 0\t    TN\t    8\t    5\t     3\n",
    "    # 1\t    FP\t    1\t    1\t     0\n",
    "    # 2\t    FN\t    0\t    0\t     0\n",
    "    # 3\t    TP\t    3\t    0\t     3\n",
    "\n",
    "    total_ct = pd.crosstab(y,y_pred)\n",
    "    total_ct = total_ct.reindex(columns=[False,True],index = [False,True],fill_value=0)\n",
    "\n",
    "    never_dep_mask = id.isin(id_status[~id_status['depart']]['id'])\n",
    "    dep_mask = id.isin(id_status[id_status['depart']]['id'])\n",
    "\n",
    "    never_dep_ct = pd.crosstab(y[never_dep_mask],y_pred[never_dep_mask])\n",
    "    never_dep_ct = never_dep_ct.reindex(columns=[False,True],index = [False,True],fill_value=0)\n",
    "\n",
    "    dep_ct = pd.crosstab(y[dep_mask],y_pred[dep_mask])\n",
    "    dep_ct = dep_ct.reindex(columns=[False,True],index = [False,True],fill_value=0)\n",
    "\n",
    "    expanded_crosstab = pd.DataFrame(\n",
    "        {\n",
    "            'class': ['TN','FP','FN','TP'],\n",
    "            'total': total_ct.values.flatten(),\n",
    "            'never dep': never_dep_ct.values.flatten(),\n",
    "            'dep': dep_ct.values.flatten()\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return(expanded_crosstab)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A false postive month just before the physician enters the \"n-month to departure\" interval probably has a different meaning than a false positive month far away from the departure date. There is a much higher rate of false positive months for physicians who go on to quit. This is likely because some features predictive of quitting are present even before our departure interval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_crosstab = expanded_crosstab(y_test,y_test_pred,X_test_ids,test_ids)\n",
    "display(ex_crosstab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ex_crosstab.iloc[1,3]/(ex_crosstab.iloc[0,3]+ex_crosstab.iloc[1,3]))\n",
    "print(ex_crosstab.iloc[1,2]/(ex_crosstab.iloc[0,2]+ex_crosstab.iloc[1,2]))\n",
    "print(ex_crosstab.iloc[1,1]/(ex_crosstab.iloc[0,1]+ex_crosstab.iloc[1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Physician level model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_physician_data(X,y,y_pred,y_prob,X_ids,ids):\n",
    "    P = X[['study_day']].copy()\n",
    "    P['id'] = X_ids\n",
    "    P['prob'] = y_prob[:,1]\n",
    "    P['pred'] = y_pred\n",
    "    P['depart'] = y\n",
    "    P['phys_depart'] = P.id.isin(ids[ids['depart']]['id'])\n",
    "    P['month_sync'] = P.groupby('id')['study_day'].transform(lambda x: round((x-max(x))/30))\n",
    "    P['prob_rm'] = P.groupby('id')['prob'].rolling(3).mean().to_list()\n",
    "    return(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_test = compile_physician_data(X_test,y_test,y_test_pred,y_test_pred_prob,X_test_ids,test_ids)\n",
    "P_train = compile_physician_data(X_train,y_train,y_train_pred,y_train_pred_prob,X_train_ids,train_ids)\n",
    "P_val = compile_physician_data(X_val,y_val,y_val_pred,y_val_pred_prob,X_val_ids,val_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(P_test, x = 'month_sync', y='prob', line_group='id',color='phys_depart')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model, and training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./models/xgb_classifier_train_without_specialty.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "    pickle.dump([classify_xgb,X_train,y_train,X_train_ids,train_ids], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shap and stuff will go elsewhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(classify_xgb, X_train, feature_perturbation='interventional', model_output=\"probability\")\n",
    "shap_values = explainer.shap_values(X_train)\n",
    "shap_obj = explainer(X_train)\n",
    "shap_interaction_values = shap.TreeExplainer(classify_xgb).shap_interaction_values(X_train)\n",
    "shap_expected = explainer.expected_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_train_sum = np.sum(shap_obj.values,axis=1)+explainer.expected_value\n",
    "\n",
    "comp_prob_shap = pd.DataFrame({\n",
    "    'xgb': y_train_pred_prob[:,1],\n",
    "    'shap': shap_train_sum\n",
    "})\n",
    "\n",
    "assert (comp_prob_shap.xgb-comp_prob_shap.shap).max() <0.00001, 'shap score mismatch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shap.initjs()\n",
    "# query_id = 298\n",
    "# id_locs = X_train_ids==query_id\n",
    "# shap.plots.force(explainer.expected_value, shap_values.values[id_locs,:],X_train.loc[id_locs],link='logit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.beeswarm(deepcopy(shap_obj), max_display=40, plot_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use this function to get a true positive sample and a true negative sample for a specific condition\n",
    "rows = []\n",
    "for select_id in set(X_train_ids):\n",
    "    # we use study day as we want to use the last study day to figure out if a person who matches the criteria\n",
    "    idx = X_train[X_train_ids == select_id]['study_day'].sort_values(ascending=False).head(1).index\n",
    "    rows.append(X_train.loc[idx])\n",
    "select_df = pd.concat(rows)\n",
    "# this is our condition\n",
    "select_df = select_df[select_df['patient_volume'] < X_train['patient_volume'].median()]\n",
    "select_df['y'] = y_train[select_df.index]\n",
    "\n",
    "tp_idx = X_train.index.tolist().index(select_df[select_df['y'] == True].sample(n=1).index)\n",
    "tn_idx = X_train.index.tolist().index(select_df[select_df['y'] == False].sample(n=1).index)\n",
    "print(tp_idx)\n",
    "print(tn_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for the figure\n",
    "median_feat = ['{}: ({})'.format(col, str( np.round(med, 1))) for col, med in zip(X_train.columns.tolist(), X_train.median())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A true positive example\n",
    "print(y_train.iloc[tp_idx])# confirm that its a true positive by looking at the y...\n",
    "plot = shap.decision_plot(explainer.expected_value,shap_values[tp_idx,:],X_train.iloc[tp_idx,:], feature_names=median_feat, link='identity', return_objects=True, feature_display_range=slice(-1, -26, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A true negative example\n",
    "print(y_train.iloc[tn_idx])\n",
    "shap.decision_plot(explainer.expected_value,shap_values[tn_idx,:],X_train.iloc[tn_idx,:],feature_names=median_feat, feature_display_range=slice(-1, -26, -1))# in parenthesis is the actual value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.loc[tn_idx]['age_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(\"physician_demand\", shap_values, X_train)\n",
    "'''\n",
    "This is a plot for risk_avg and its interaction with age_group. Here we see that with low risk_avg our model tends to push\n",
    "towards a 0 and as risk_avg goes up it tends to not push the model towards a 1 even if the physician is older.\n",
    "We can also see that at age_group high and risk_avg==2 (complexity) there is a lot of interaction and if the age group is high then they are more likely to depart (1)\n",
    "This makes sense because we can see that in the bswarm plot if there is low complexity (ris_avg == 0) its more protective, \n",
    "while middle risk average is more indicative of a 1 (depart) and high risk_avg doesnt really push the model towards a 1 or 0\n",
    "so a physician with low patient complexity and high age may be indicative of someone retiring?\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(\"risk_avg\", shap_values, X_train)\n",
    "'''\n",
    "This is a plot for risk_avg and its interaction with age_group. Here we see that with low risk_avg our model tends to push\n",
    "towards a 0 and as risk_avg goes up it tends to not push the model towards a 1 even if the physician is older.\n",
    "We can also see that at age_group high and risk_avg==2 (complexity) there is a lot of interaction and if the age group is high then they are more likely to depart (1)\n",
    "This makes sense because we can see that in the bswarm plot if there is low complexity (ris_avg == 0) its more protective, \n",
    "while middle risk average is more indicative of a 1 (depart) and high risk_avg doesnt really push the model towards a 1 or 0\n",
    "so a physician with low patient complexity and high age may be indicative of someone retiring?\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(\"panel_cnt\", shap_values, X_train)\n",
    "'''\n",
    "Here panel count is interesting because as we move up in panel count the probability starts off as high for\n",
    "departing (target variable 1) and as it increases to about 1000 it switches to a protective feature \n",
    "where as the panel count increases the probability of departure decreases\n",
    "\n",
    "This plot shows that the variable it interacts the most with is tenure. For tenure we see that with mid to large panel count\n",
    "we have some high tenure folk showing up. This may imply that the folks who have high panel count and high tenure interaction tend to be \n",
    "protective (retained) we can also see that high tenure does not really impact the model too much\n",
    "Positive SHAP value means positive impact on prediction, leading the model to predict 1\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.scatter(shap_values[:,'age_group'], color = shap_values[:,'risk_avg']) # male is 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_interaction_values = shap.TreeExplainer(classify_xgb).shap_interaction_values(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_interaction_values, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(('age_group','gender'), shap_interaction_values, X_train, display_features=X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "cdict1 = {\n",
    "    'red': ((0.0, 0.11764705882352941, 0.11764705882352941),\n",
    "            (1.0, 0.9607843137254902, 0.9607843137254902)),\n",
    "\n",
    "    'green': ((0.0, 0.5333333333333333, 0.5333333333333333),\n",
    "              (1.0, 0.15294117647058825, 0.15294117647058825)),\n",
    "\n",
    "    'blue': ((0.0, 0.8980392156862745, 0.8980392156862745),\n",
    "             (1.0, 0.3411764705882353, 0.3411764705882353)),\n",
    "\n",
    "    'alpha': ((0.0, 1, 1),\n",
    "              (0.5, 1, 1),\n",
    "              (1.0, 1, 1))\n",
    "}  # #1E88E5 -> #ff0052\n",
    "red_blue_solid = LinearSegmentedColormap('RedBlue', cdict1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_embedded = TSNE(n_components=2, perplexity=75).fit_transform(shap_values.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "f = plt.figure(figsize=(10,10))\n",
    "plt.scatter(shap_embedded[:,0],\n",
    "           shap_embedded[:,1],\n",
    "           c=explainer.expected_value+shap_values.values.sum(1).astype(np.float64),\n",
    "           linewidth=0, alpha=1., cmap=red_blue_solid)\n",
    "for id in train_ids[train_ids.depart]['id']:\n",
    "    cur_ids = X_train_ids==id\n",
    "    plt.plot(shap_embedded[cur_ids,0],\n",
    "           shap_embedded[cur_ids,1],\n",
    "           c='#555555',\n",
    "           linewidth = 2,\n",
    "           alpha = 0.5\n",
    "    )\n",
    "# only log odds when you set shap to have raw (its an xgboost thing)\n",
    "cb = plt.colorbar(label=\"Probability departure_in_interval (6 months)\", aspect=40, orientation=\"horizontal\")\n",
    "cb.set_alpha(1)\n",
    "cb.draw_all()\n",
    "cb.outline.set_linewidth(0)\n",
    "cb.ax.tick_params('x', length=0)\n",
    "cb.ax.xaxis.set_label_position('top')\n",
    "plt.gca().axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id_status[test_id_status.depart]['id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = np.array(X_test.study_day/X_test.study_day.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = ['age_group',\n",
    " 'gender',\n",
    " 'calendar_month',\n",
    " 'covid_wave',\n",
    " 'patient_volume',\n",
    " 'physician_demand',\n",
    " 'physician_work_intensity',\n",
    " 'panel_cnt',\n",
    " 'risk_avg',\n",
    " 'teamwork_on_inbox_value',\n",
    " 'note_quality_manual_value',\n",
    " 'note_quality_contribution_value',\n",
    " 'number_of_rx_errors',\n",
    " 'ehr_time_8',\n",
    " 'wow_time_8',\n",
    " 'note_time_8',\n",
    " 'order_time_8',\n",
    " 'ib_time_8',\n",
    " 'review_time_8',\n",
    " 'tenure',\n",
    " 'study_day',\n",
    " 'specialty_Cardiovascular Disease',\n",
    " 'specialty_Endocrinology, Diabetes & Metabolism',\n",
    " 'specialty_Family Medicine',\n",
    " 'specialty_Gastroenterology',\n",
    " 'specialty_Internal Medicine',\n",
    " 'specialty_Obstetrics and Gynecology',\n",
    " 'specialty_Other Medical Subspecialty',\n",
    " 'specialty_Pediatrics',\n",
    " 'specialty_Pulmonary Disease',\n",
    " 'specialty_Rheumatology',\n",
    " 'specialty_Surgery',\n",
    " 'specialty_Surgical Subspecialty',\n",
    " 'EWA_avg_patient_volume',\n",
    " 'EWA_avg_physician_demand',\n",
    " 'EWA_avg_physician_work_intensity',\n",
    " 'EWA_avg_panel_cnt',\n",
    " 'EWA_avg_risk_avg',\n",
    " 'EWA_avg_teamwork_on_inbox_value',\n",
    " 'EWA_avg_note_quality_manual_value',\n",
    " 'EWA_avg_note_quality_contribution_value',\n",
    " 'EWA_avg_number_of_rx_errors',\n",
    " 'EWA_avg_ehr_time_8',\n",
    " 'EWA_avg_wow_time_8',\n",
    " 'EWA_avg_note_time_8',\n",
    " 'EWA_avg_order_time_8',\n",
    " 'EWA_avg_ib_time_8',\n",
    " 'EWA_avg_review_time_8',\n",
    " 'r_slope_patient_volume',\n",
    " 'r_slope_physician_demand',\n",
    " 'r_slope_physician_work_intensity',\n",
    " 'r_slope_panel_cnt',\n",
    " 'r_slope_risk_avg',\n",
    " 'r_slope_teamwork_on_inbox_value',\n",
    " 'r_slope_note_quality_manual_value',\n",
    " 'r_slope_note_quality_contribution_value',\n",
    " 'r_slope_number_of_rx_errors',\n",
    " 'r_slope_ehr_time_8',\n",
    " 'r_slope_wow_time_8',\n",
    " 'r_slope_note_time_8',\n",
    " 'r_slope_order_time_8',\n",
    " 'r_slope_ib_time_8',\n",
    " 'r_slope_review_time_8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst2 = ['prov_id', 'date_yyyymm_x', 'rv_us', 'mean', 'merge_id', 'date_yyyymm_y', 'reporting_period_start_date', 'reporting_period_end_date', 'specialty', 'prov_type', 'age_as_of_06_30_2021', 'gender', 'hire_date', 'term_date', 'sched_time_avail_in_hrs', 'sched_time_appts_in_hrs', 'sched_time_compl_in_hrs', 'actual_prov_visit_time_in_hrs', 'time_on_unscheduled_days_num', 'time_on_unscheduled_days_denom', 'time_on_unscheduled_days_value', 'time_in_notes_per_day_num', 'time_in_notes_per_day_denom', 'time_in_notes_per_day_value', 'time_outside_scheduled_hours_num', 'time_outside_scheduled_hours_denom', 'time_outside_scheduled_hours_value', 'time_in_orders_per_day_num', 'time_in_orders_per_day_denom', 'time_in_orders_per_day_value', 'time_in_clinical_review_per_day_num', 'time_in_clinical_review_per_day_denom', 'time_in_clinical_review_per_day_value', 'time_in_in_basket_per_day_num', 'time_in_in_basket_per_day_denom', 'time_in_in_basket_per_day_value', 'orders_with_team_contributions_num', 'orders_with_team_contributions_denom', 'orders_with_team_contributions_value', 'time_in_system_per_day_num', 'time_in_system_per_day_denom', 'time_in_system_per_day_value', 'patient_volume', 'number_of_clinical_hours_scheduled', 'physician_demand', 'physician_work_intensity', 'panel_cnt', 'risk_avg', 'teamwork_on_inbox_num', 'teamwork_on_inbox_denom', 'teamwork_on_inbox_value', 'note_quality_manual_num', 'note_quality_manual_denom', 'note_quality_manual_value', 'note_quality_contribution_num', 'note_quality_contribution_denom', 'note_quality_contribution_value', 'total_time_on_ehr_outside_of_scheduled_num', 'total_time_on_ehr_outside_of_scheduled_denom', 'total_time_on_ehr_outside_of_scheduled_value', 'inbox_volume_total_messages_num', 'inbox_volume_total_messages_denom', 'inbox_volume_total_messages_value', 'number_of_rx_errors', 'physician_id', 'ehr_time_8', 'wow_time_8', 'note_time_8', 'order_time_8', 'ib_time_8', 'review_time_8', 'time_to_departure', 'departure_in_interval', 'tenure', 'study_day', 'calendar_month', 'age_group', 'covid_wave', 'EWA_avg_patient_volume', 'EWA_avg_physician_demand', 'EWA_avg_physician_work_intensity', 'EWA_avg_panel_cnt', 'EWA_avg_risk_avg', 'EWA_avg_teamwork_on_inbox_value', 'EWA_avg_note_quality_manual_value', 'EWA_avg_note_quality_contribution_value', 'EWA_avg_number_of_rx_errors', 'EWA_avg_ehr_time_8', 'EWA_avg_wow_time_8', 'EWA_avg_note_time_8', 'EWA_avg_order_time_8', 'EWA_avg_ib_time_8', 'EWA_avg_review_time_8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst2.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lst2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(lst) - (set(lst2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aebeff7af16a3e74dfb8d2ca55a8624561b514dde17729b916609a121bafd8b7"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
